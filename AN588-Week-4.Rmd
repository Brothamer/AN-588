---
title: "AN5888-Week4-notes"
author: "Brooke Rothamer"
date: "2023-09-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Module 08: Probilities and Distributions

When we do statistical inference we are basically trying to draw conclusions about a population based on measurements from a noisy sample or trying to evaluate whether it is reasonable to assume that our sample is drawn from a particular population.

This process of trying to draw conclusions is complicated by the fact that…

* our sample may be biased, non-random, or non-representative in some way
* there may be unknown or unobserved variables that impact how the sample is related to the population
* the assumptions we make about the population that our sample is drawn from might not be correct

## Probability

The term probability is applied to population level variables that describe the magnitude of chance associated with particular observations or event. Probabilities summarize the relative frequencies of possible outcomes. Probabilities are properties of distributions. Probabilities vary between zero and one. Outcomes that are impossible have Pr = 0, those that are certain have Pr = 1.

### Example: Rolling a Die
We will use the {manipulate} package and the sample() function to explore the effects of sample size on estimates of the probability of different outcomes. The probability of each outcome (rolling a “1”, “2”,…, “6”) is 1 in 6, but our estimate of the probability of each possible outcome will change with sample size.

```{r die rolls, eval=FALSE}
library(manipulate)
outcomes <- c(1, 2, 3, 4, 5, 6)
manipulate(hist(sample(outcomes, n, replace = TRUE), breaks = c(0.5, 1.5, 2.5,
    3.5, 4.5, 5.5, 6.5), probability = TRUE, main = paste("Histogram of Outcomes of ",
    n, " Die Rolls", sep = ""), xlab = "roll", ylab = "probability"), n = slider(0,
    10000, initial = 100, step = 100))
```
Note: you must run manipulate directly in the console and manipulate the plot settings in the plots tab to change the number of rolls

### Challenge 1

Write a function to simulate rolling a die where you pass the number of rolls as an argument. Then, use your function to simulate rolling two dice 1000 times and take the sum of the rolls. Plot a histogram of those results.

```{r Challenge 1}
nrolls <- 1000 #just saving the number of rolls as a variable for ease
roll <- function(x) {sample(1:6, x, replace = TRUE)} #creating a simple function that simulates rolling a 6-sided die
two_dice <- roll(nrolls) + roll(nrolls) #combining functions to simulate rolling two dice 1000 (nrolls) times and summing the outcomes each time
hist(two_dice, breaks = c(1.5:12.5), probability = TRUE, main = "Rolling Two Dice", xlab = "sum of rolls", ylab = "probability") #creating a histogram of the outcomes
```

Note: this histogram will look a little different every time the code is run because it depends on probability, but it should look similar overall.

### Rules of Probability

1. Pr(+) = Probablility that something occurs = 1
2. Pr(∅) = Probability that nothing occurs = 0
3. Pr(A) = Probability that a particular event A occurs
  + 0 ≤ Pr(A) ≤ 1
4.Pr (A ⋃ B) = Probability that a particular event A ***or*** a particular event B occurs = UNION.
  + If events A and B are mutually exclusive, then this is equal to Pr(A) + Pr(B)
5. Pr (A ⋂ B) = Probability that a particular event A ***and*** a particular event B occur = INTERSECTION
  + Pr(A ⋂ B) = Pr(A | B) × Pr(B) = Pr(B | A) × Pr(A)
  + If the events are independent, then this is equal to Pr(A) × Pr(B)
  + If Pr (A ⋂ B) = 0, then the events are mutually exclusive
6. Pr( = Probability of the completement of A = 1 - Pr(A)
7. Conditional Probability is the probability of an event occuring after taking into accoun the occurence of another event
  + For example, the probability of a die coming up as a “1” given that we know the die came up as an odd number (“1”, “3”, or “5”).
  + Pr(A | B) = Pr(A ⋂ B) ÷ Pr(B)
  + If events A and B are independent, then Pr(A | B) = [Pr(A) ×
  Pr(B)] ÷ Pr(B) = Pr(A)
  + If events A and B are dependent, then Pr(A | B) ≠ Pr (A)
  
### Challenge 2

You have a deck of 52 cards, Ace to 10 + 3 face cards in each suit. You draw a card at random.

```{r Challenge 2}
deck <- 52
face <- 3*4
king <- 4
suit <- 13
red <- 2 * suit
```

* What is the probability that you draw a face card?

```{r Challenge 2 part 2}
faceprob <- face/deck
faceprob
```
* What is the probability that you draw a King?
```{r Challenge 2 part 3}
kingprob <- king/deck
kingprob
```
* What is the probability that you draw a spade?
```{r Challenge 2 part 4}
suitprob <- suit/deck
suitprob
```
* What is the probability that you draw a spade given that you draw a face card?
  + Conditional Probability; independent
```{r Challenge 2 part 5}
spadefaceprob <- suit/deck
spadefaceprob
```
* What is the probability that you draw a King given that you draw a face card?
  + Conditional Probability; dependent
```{r Challenge 2 part 6}
kingfaceprob <- king/face
kingfaceprob
```
* What is the probability that you draw a card that is both from a red suit (hearts or diamonds) and a face card?
  + Intersection
```{r Challenge 2 part 7}
faceredprob <- (red/deck) * (face/deck)
faceredprob
```

## Random Variables

A **random variable** is a variable whose outcomes are assumed to arise by chance or according to some random or stochastic mechanism. The chances of observing a specific outcome or an outcome value within a specific interval has associated with it a **probability**.

1. **Discrete Random Variables** are random variables that can assume only a countable number of discrete possibilities (e.g., counts of outcomes in a particular category). We can assign a probability to each possible outcome.

2. **Continuous Random Variables** are random variables that can assume any real number value within a given range (e.g., measurements). We cannot assign a specific probability to each possible outcome value as the set of possible outcomes is infinite, but we can assign probabilities to intervals of outcome values.
* example: measuring a distance

A **probability function** is a mathematical function that describes the chance associated with a random variable having a particular outcome or falling within a given range of outcome values.

1. **Probability Mass Functions (PMFs)** are associated with *discrete* random variables. These functions describe the probability that a random variable takes a particular discrete value.

* There are k distinct outcomes x1,x2,...,xk
* 0 ≤ Pr (X=xi) ≤ 1 for all xi
* ∑ Pr (X=xi) for all x from x1 to xk = 1

Example: Flipping a coin
```{r Random variables 1}
outcomes <- c("heads", "tails") #assigns outcomes as characters
prob <- c(1/2, 1/2) #assigns probability of each outcome
barplot(prob, ylim = c(0, 0.6), names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Pr(X = outcome)", main = "Probability Mass Function") #make a plot of an example of what probability mass functions produce
```
* If we have a large enough sample, this is the distribution we expect to see
  
2. **Probability Density Functions (PDFs)** are associated with *continuous* random variables. These functions describe the probability that a random variable falls within a given range of outcome values. The probability associated with that range equals the area under the density function for that range.

  * f(x) ≥ 0 for all −∞ ≤ x ≤ +∞ (non-negative everywhere)
  * The total area under f(x) = 1

Example: Beta distribution

* The **Beta Distribution** refers to a family of continuous probability distributions defined over the interval [0, 1] and parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable x and control the shape of the distribution.

  f(x) = Kx^(α−1) (1−x)^(β−1)

If we set K = 2, α = 2, and β = 1 and restrict the domain of x to [0, 1], it gives us a triangular function that we can graph as follows:

```{r Random Variables 2}
library(ggplot2)
a <- 2
b <- 1
K <- 2
x <- seq(from = 0, to = 1, by = 0.025) #counting off by 0.025
fx <- K * x^(a - 1) * (1 - x)^(b - 1) #the function
lower_x <- seq(from = -0.25, to = 0, by = 0.025)  # add some values of x less than zero
upper_x <- seq(from = 1, to = 1.25, by = 0.025)  # add some values of x greater than one
lower_fx <- rep(0, 11)  # add fx=0 values to x<0
upper_fx <- rep(0, 11)  # add fx=0 values to x>1
x <- c(lower_x, x, upper_x)  # paste xs together
fx <- c(lower_fx, fx, upper_fx)  # paste fxs together
d <- as.data.frame(cbind(x, fx))
p <- ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") + geom_line()
p
```

We can use manipulate to show this interactively. Reminder that manipulate has to be entered directly in the console.

```{r Random Variables 3, eval=FALSE}
library(manipulate)
library(ggplot2)
manipulate(plot(1:5, cex=size), size = slider(0.5,10,step=0.5)) #use this if the slider/settings option doesn't show up in the plots pane
manipulate(ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") +
    geom_line() + geom_polygon(data = data.frame(xvals = c(0, n, n, 0), fxvals = c(0,
    K * n^(a - 1) * (1 - n)^(b - 1), 0, 0)), aes(x = xvals, y = fxvals)) + ggtitle(paste("Area Under Function = ",
    0.5 * n * K * n^(a - 1) * (1 - n)^(b - 1), sep = " ")), n = slider(0, 1,
    initial = 0.5, step = 0.01))
```

The shaded area represents the **cumulative probability** integrated across f(x) from −inf to x.

The **cumulative distribution function**, or **CDF**, of a random variable is defined as the probability of observing a random variable X taking the value of x or less, i.e., F(x) = Pr (X ≤ x).

* This definition applies regardless of whether X is discrete or continuous. Note here we are using F(x) for the cumulative distribution function rather than f(x), which we use for the probability density function. For a continuous variable, the PDF is simply the first derivative of the CDF, i.e., f(x) = d F(x)

```{r Random Variables 4}
x <- seq(from = 0, to = 1, by = 0.005)
prob <- 0.5 * x * K * x^(a - 1) * (1 - x)^(b - 1)
barplot(prob, names.arg = x, space = 0, main = "Cumulative Probability", xlab = "x",
    ylab = "Pr(X ≤ x)")
```

The built in R function for the Beta Distribution, pbeta(), can give us the cumulative probability directly, if we specify the values of α = 2 and β = 1.

```{r Random Variables 5}
pbeta(0.75, 2, 1)  # cumulative probability for x ≤ 0.75

pbeta(0.5, 2, 1)  # cumulative probability for x ≤ 0.50
```

In general, we find the cumulative probability for a continuous random variable by calculating the area under the probability density function of interest from −∞ to x. This is what is is being returned from pbeta().

The other related **Beta Distribution** functions, e.g., rbeta(), dbeta(), and qbeta(), are also useful. rbeta() draws random observations from a specfied beta distribution. dbeta() gives the point estimate of the beta density function at the value of the argument x, and qbeta() is essentially the converse of pbeta(), *i.e., it tells you the value of x that is associated with a particular cumulative probability, or quantile, of the cumulative distribution function. Other PMFs and PDFs have comparable r, d, p, and q functions.*

We can define the **survival function** for a random variable X as S(x) = Pr (X > x) = 1 - Pr (X ≤ x) = 1 - f(x)
* survival function as in surviving past that point

Finally, we can define the *“qth”" quantile of a cumulative distibution function* as the value of x at which the CDF has the value “q”, i.e., F(xq)=q.

### Expected Mean and Variance of Random Models

The mean value (or expectation) and the expected variance for a random variable with a given probability mass function can be expressed generally as follows:

μX = Expectation for X = ∑ xi × Pr (X=xi) for all x from xi to xk
* xi is a random choice of number within that distribution

σ2X = Variance of X = ∑ (xi−μX)2 × Pr (X=xi) for all x from xi to xk

Applying these formulae to die rolls, we could calculate the expectation for X for a large set of die rolls…

(1 * 1/6) + (2 * 1/6) + … + (6 * 1/6) = 3.5

```{r expected mean}
m <- sum(seq(1:6) * 1/6) #this is the basic algebraic mean of the integers 1 through 6 which should represent any (large) number of dice rolls
m
```

And the expected variance…

[(1 - 3.5)^2 * (1/6)] + [(2 - 3.5)^2 * (1/6)] + … + [(6 - 3.5)^2 * (1/6)] =

```{r expected variance}
var <- sum((seq(1:6) - mean(seq(1:6)))^2 * (1/6)) #this is the basic variance calculation if the data were just the integers 1 through 6
var
```

Likewise, we can calculate the expectation and variance for a random varible X
 with a given probability density function generally as follows:
 μX = Expectation for X = ∫+∞−∞ x f(x) dx

σ2X = Variance of X = ∫+∞−∞ (x−μX)2 f(x) dx

To demonstrate these numerically would require a bit of calculus, i.e., integration.

## Useful Probability Distributions for Random Variables

### Probability Mass Functions

The Bernoulli Distribution is the probability distribution of a *binary* random variable, i.e., a variable that has only two possible outcomes, such as *success or failure*, *heads or tails*, *true or false*. If p is the probability of one outcome, then 1−p has to be the probabilty of the alternative. For flipping a fair coin, for example, p = 0.5 and 1−p also = 0.5.

For the BERNOULLI DISTRIBUTION, the probability mass function is:

  f(x) = p^x (1−p)^(1−x) where x = {0 or 1}

For this distribution, μX = p and σ2X = p(1−p)

### Challenge 3

Using the Bernoulli distribution, calculate the expectation for drawing a spade from a deck of cards? What is the variance in this expectation across a large number of draws?

```{r challenge 3 part 1}
suit <- 13
deck <- 52
PrSpade <- (suit/deck)^1 * ((deck-suit)/deck)^0
PrSpade
```
* Pr (spade) = (13/52)^1 × (39/52)^0 = 0.25

```{r challenge 3 part 2}
VarSpade <- (suit/deck) * (1 - (suit/deck))
VarSpade
```
* Var (spade) = (13/52) × (1−13/52) = (0.25) × (0.75) = 0.1875

The Bernoulli distribution is a special case of the **Binomial Distribution**. The binomial distribution is typically used to model the probability of a number of “successes” k out of a set of “trials” n, i.e., for *counts* of a particular outcome.

Again, the probability of success on each trial = p and the probability of not success = 1−p.

For the BINOMIAL DISTRIBUTION, the probability mass function is:

  f(x) = (nk) p^k (1-p)^(n-k)

where x = {0, 1, 2, … , n} and where

  (nk) = n! / (k!(n-k)!)
  
This is read as “n choose k”, i.e., the probability of k successes out of n trials. This is also called the “binomial coefficient”.

For this distribution, μX = np and σ2X = np(1-p). Recall, μX = expected number of successes in n trials

Where n = 1, this simplifies to the Bernoulli distribution.

### Challenge 4

What is the chance of getting a “1” on each of six consecutive rolls of a die?

```{r challenge 4 part 1}
n <- 6  #number of trials
k <- 6  #number of successes
p <- 1/6 #probability of each success
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - k)
prob
```
What about of getting exactly three “1”s?
```{r challenge 4 part 2}
k <- 3  #number of successes; n and p same as last time
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - k)
prob
```
or R has a built in density function dbinom()

```{r challenge 4 part 2b}
dbinom(x = k, size = n, prob = p) #there is a binomial distribution function built into R
```

What is the expected number of “1”s to occur in six consecutive rolls?

```{r challenge 4 part 3}
dbinom(x = 6, size = 6, prob = 1/6) #x = the number of successes
```
  * We can use the built in function pbinom() to return the value of the cumulative distribution function for the binomial distribution, i.e., the probability of observing up to and including a given number of successes in n trials.

The chances of observing exactly 0, 1, 2, 3, … 6 rolls of “1” on 6 rolls of a die are…

```{r challenge 4 part 4}
probset <- dbinom(x = 0:6, size = 6, prob = 1/6)  # x is number of successes, size is number of trials
barplot(probset, names.arg = 0:6, space = 0, xlab = "outcome", ylab = "Pr(X = outcome)",
    main = "Probability Mass Function")
```

The **Poisson Distribution** is often used to model open ended counts of independently occuring events, for example the number of cars that pass a traffic intersection over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The probability mass function for the Poisson distribution is described by a single parameter, λ, where λ can be interpreted as the mean number of occurrences of the event in the given interval.

The probability mass function for the POISSON DISTRIBUTION is:
  
  f(x) = (λ^x exp(-λ))/x!
  
where x = {0, 1, 2, …}

For this distribution, μX = λ and vX = λ

*Note that the mean and the variance are the same!*

Let’s use R to look at the probability mass functions for different values of λ:
```{r Poisson distribution 1}
x <- 0:10
l = 3.5
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```
```{r Poisson distribution 2}
x <- 0:20
l = 10
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

As we did for other distributions, we can also use the built in probability function for the Poisson distribution, ppois(), to return the value of the **cumulative distribution function**, i.e., the probability of observing up to and including a specific number of events in the given interval.

```{r Poisson distribution 3}
x <- 0:10
l <- 3.5
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x",
    ylab = "Pr(X ≤ x)", main = "Cumulative Probability")
```

### Probability Density Functions

The **Uniform Distribution** is the simplest probability density function describing a continuous random variable. The probability is uniform and does not fluctuate across the range of x values in a given interval.

The probability density function for the UNIFORM DISTRIBUTION is:

  f(x) = 1/(b-a)
  
where a ≤ x ≤ b and 0 for x < a and x > b

### Challenge 5
What would you predict the expectation (mean) should be for a uniform distribution?

For this distribution:

  (mu)x = (a+b)/2           This is the mean of 2 numbers a and b
  (sigma)^2x = (b-a)^2/12   This is the standard dev of a and b
  
Let’s plot a uniform distribution across a given range, from a = 4 to b = 8…

```{r Challenge 5 part 1}
a <- 4
b <- 8
x <- seq(from = a - (b - a), to = b + (b - a), by = 0.01)
fx <- dunif(x, min = a, max = b)  # dunif() evaluates the density at each x
plot(x, fx, type = "l", xlab = "x", ylab = "f(x)", main = "Probability Density Function")
```
* Yup, its uniform; the values are between 4 and 8

Note that for the uniform distribution, the cumulative density function increases linearly over the given interval.

```{r Challenge 5 part 2}
plot(x, punif(q = x, min = a, max = b), type = "l", xlab = "x", ylab = "Pr(X ≤ x)",
    main = "Cumulative Probability")  # punif() is the cumulative probability density up to a given x
```

### Challenge 6

plot(x, punif(q = x, min = a, max = b), type = "l", xlab = "x", ylab = "Pr(X ≤ x)", main = "Cumulative Probability")  # punif() is the cumulative probability density up to a given x

  * i don't know; need to figure this out

```{r Challenge 6}

```

The **Normal** or **Gaussian Distribution** is perhaps the most familiar and most commonly applied probability density functions for modeling continuous random variables. Why is the normal so important? Many traits are normally distributed, and the additive combination of many random factors is also commonly normally distributed.

Two parameters, μ and σ, are used to describe a normal distribution.

We can get an idea of the shape of a normal distribution with different μ and σ using the simple R code below. Try playing with μ and σ.

```{r normal distribution}
mu <- 4
sigma <- 1.5
curve(dnorm(x, mu, sigma), mu - 4 * sigma, mu + 4 * sigma, main = "Normal Curve",
    xlab = "x", ylab = "f(x)")
```
The function, dnorm() gives the point value of the normal density function at a given value of x. x can range from -∞ to +∞. Recall, it does not make sense to talk about the “probability” associated with a given value of x as this isa density not a mass functions, but we can talk about the probability of x falling within a given interval.

The code below lets you play interactively with μ
, σ
, and nsigma (which shades in the proportion of the distribution falling within that number of standard deviations of the mean). Also, look carefully at the code to try to figure out what each bit is doing. (Be sure to put it directly into the console.)

```{r normal distribution 2, eval=FALSE}
manipulate(plot(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
    dnorm(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(mu - 4 * sigma, mu + 4 *
        sigma), xlab = "x", ylab = "f(x)", main = "Normal Probability Density Function") +
    polygon(rbind(c(mu - nsigma * sigma, 0), cbind(seq(from = (mu - nsigma *
        sigma), to = (mu + nsigma * sigma), length.out = 1000), dnorm(seq(from = (mu -
        nsigma * sigma), to = (mu + nsigma * sigma), length.out = 1000), mean = mu,
        sd = sigma)), c(mu + nsigma * sigma, 0)), border = NA, col = "salmon") +
    abline(v = mu, col = "blue") + abline(h = 0) + abline(v = c(mu - nsigma *
    sigma, mu + nsigma * sigma), col = "salmon"), mu = slider(-10, 10, initial = 0,
    step = 0.25), sigma = slider(0.25, 4, initial = 1, step = 0.25), nsigma = slider(0,
    4, initial = 0, step = 0.25))
```

The pnorm() function, as with the p- variant function for other distributions, returns the cumulative probability of observing a value less than or equal to x, i.e., Pr (X ≤ x). Type it the code below and then play with values of μ and σ to look at how the cumulative distibution function changes.

```{r normal distribution 3, eval=FALSE}
manipulate(plot(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000),
    pnorm(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(-20, 20), xlab = "x", ylab = "f(x)",
    main = "Cumulative Probability"), mu = slider(-10, 10, initial = 0, step = 0.25),
    sigma = slider(0.25, 10, initial = 1, step = 0.25))  # plots the cumulative distribution function
```

You can also use pnorm() to calculate the probability of an observation drawn from the population falling within a particular interval. For example, for a normally distributed population variable with μ = 6 and σ = 2, the probability of a random observation falling between 7 and 8 is…
```{r normal distribution 4, eval=FALSE}
p <- pnorm(8, mean = 6, sd = 2) - pnorm(7, mean = 6, sd = 2)
p
```
Likewise, you can use pnorm() to calculate the probability of an observation falling, for example within 2 standard deviations of the mean of a particular normal distribution.
```{r normal distribution 5, eval=FALSE}
mu <- 0
sigma <- 1
p <- pnorm(mu + 2 * sigma, mean = mu, sd = sigma) - pnorm(mu - 2 * sigma, mean = mu,
    sd = sigma)
p
```
Regardless of the specific values of μ and σ, about 95% of the normal distribution falls within 2 standard deviations of the mean and about 68% of the distribution falls within 1 standard deviation.
```{r normal distribution 6, eval=FALSE}
p <- pnorm(mu + 1 * sigma, mean = mu, sd = sigma) - pnorm(mu - 1 * sigma, mean = mu,
    sd = sigma)
p
```
Another one of the main functions in R for probability distributions, the qnorm() function, will tell us the value of x below which a given proportion of the cumulative probability function falls. As we saw earlier, too, we can use qnorm() to calculate confidence intervals. The code below

```{r normal distribution 7, eval=FALSE}
manipulate(plot(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
    dnorm(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(mu - 4 * sigma, mu + 4 *
        sigma), xlab = "x", ylab = "f(x)", main = "Normal Probability Density Function") +
    abline(v = mu, col = "blue") + abline(h = 0) + polygon(x = c(qnorm((1 -
    CI)/2, mean = mu, sd = sigma), qnorm((1 - CI)/2, mean = mu, sd = sigma),
    qnorm(1 - (1 - CI)/2, mean = mu, sd = sigma), qnorm(1 - (1 - CI)/2, mean = mu,
        sd = sigma)), y = c(0, 1, 1, 0), border = "red"), mu = slider(-10, 10,
    initial = 0, step = 0.25), sigma = slider(0.25, 10, initial = 1, step = 0.25),
    CI = slider(0.5, 0.99, initial = 0.9, step = 0.01))
```

### Challenge 7

* Create a vector, v, containing n random numbers selected from a normal distribution with mean μ and standard deviation σ. Use 1000 for n, 3.5 for μ, and 4 for σ. HINT: Such a function exists! rnorm().
* Calculate the mean, variance, and standard deviation for your sample of random numbers.
* Plot a histogram of your random numbers.

```{r Challenge 7}
n <- 1000
mu <- 3.5
sigma <- 4
v <- rnorm(n, mu, sigma)
mean(v)
var(v)
sd(v)
hist(v, breaks = seq(from = -15, to = 20, by = 0.5), probability = TRUE)
```

A quantile-quantile or “Q-Q” plot can be used to look at whether a set of data seem to follow a normal distribution. A Q–Q plot is a graphical method for generally comparing two probability distributions. To examine a set of data for normality graphically, you plot the quantiles for your actual data (as the y values) versus the theoretical quantiles (as the x values) pulled from a normal distribution. If the two distributions being compared are similar, the points in the plot will approximately lie on the line y = x.

In this case, this should be apparent since you have simulated a vector of data from a distribution normal distribution.

To quickly do a Q-Q plot, call the two R functions qqnorm() and qqline() using the vector of data you want to examine as an argument.

```{r quantile-quantile}
qqnorm(v, main = "Normal QQ plot random normal variables")
qqline(v, col = "gray")
```

### Challenge 8

What happens if you simulate fewer observations in your vectors? Or if you simulate observations from a different distribution?

The “Standard Normal” Distribution
Any normal distribution with mean μ and standard deviation σ can be converted into what is called the **standard normal** distribution, where the mean is zero and the standard deviation is 1. This is done by subtracting the mean from all observations and dividing these differences by the standard deviation. The resultant values are referred to a Z scores, and they reflect the number of standard deviations an observation is from the mean.

```{r challenge 8 part 1}
x <- rnorm(10000, mean = 5, sd = 8)  # simulate from a normal distribution with mean 5 and SD 8
hist(x)
```

```{r challenge 8 part 1b}
mean(x)
sd(x)
```
Essentially 5 and 8, so:
```{r challenge 8 part 2}
z <- (x - mean(x))/sd(x)  # standardized!
hist(z)
```
```{r challenge 8 part 2b}
mean(z)
sd(z)
```
Now very close to 0 and 1

## Sample Distributions versus Population Distributions

It is important to recognize that, above, we were dealing with probability distributions of discrete and continuous random variables as they relate to populations. But, as we have talked about before, we almost never measure entire populations; instead, we measure samples from populations and we characterize our samples using various statistics. The theoretical probability distributions described above (and others) are models for how we connect observed sample data to populations, taking into account various assumptions, and this is what allows us to do many types of inferential statistics. The most fundamental assumption is that the observations we make are independent from one another and are identically distributed, an assumption often abbreviated as iid. Obvious cases of violation of this assumption are rife in the scientific literature, and we should always be cautious about making this assumption!

The important thing for us to know is that we can get unbiased estimates of population level parameters on the basis of sample statistics.

Let’s imagine a population of 1 million zombies whose age at zombification is characterized by a normal distribution with a mean of 25 years and a standard deviation of 5 years. Below, we set up our population:

```{r zombies 1}
set.seed(1)
x <- rnorm(1e+06, 25, 5)
hist(x, probability = TRUE)
```

```{r zombies 2}
mu <- mean(x)
mu

sigma <- sqrt(sum((x - mean(x))^2)/length(x))
```

Note: We don’t use the sd() function as this would divide by length(x)-1. Check that out using sd(x)

Suppose we now sample the zombie population by trapping sets of zombies and determining the mean age in each set. We sample without replacement from the original population for each set. Let’s do that 100 times with samples of size 5 and store these in a list.

```{r zombies 3}
k <- 1000  # number of samples
n <- 5  # size of each sample
s <- NULL  # dummy variable to hold each sample
for (i in 1:k) {
    s[[i]] <- sample(x, size = n, replace = FALSE)
}
head(s)
```
For each of these samples, we can then calculate a mean age, which is a statistic describing each sample. That statistic itself is a random variable with a mean and distribution. This is the sampling distribution. How does the sampling distribution compare to the population distribution? The mean of the two is pretty close to the same! The sample mean - which is an average of the set of sample averages - is an unbiased estimator for the population mean.

```{r zombies 4}
m <- NULL
for (i in 1:k) {
    m[i] <- mean(s[[i]])
}
mean(m)  # almost equal to...
```
Again, this is the mean of the sampling distribution, which is simply the average of the means of each sample. This value should be really close to the population mean.

And oh look, it's very close to mu!

### The Standard Error
